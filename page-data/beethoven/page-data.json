{"componentChunkName":"component---src-templates-blog-post-tsx-content-file-path-home-runner-work-spelkington-github-io-spelkington-github-io-content-blog-beethoven-index-mdx","path":"/beethoven/","result":{"data":{"site":{"siteMetadata":{"title":"Chaotic Good Computing"}},"mdx":{"id":"380ff55b-2278-56c9-877f-27c69bf1481e","excerpt":"TODO: Introduction From the original submission:\n\nInspiration\n\nSpencer, one of our team members, has a parent who is hearing-impaired and another that providesâ€¦","frontmatter":{"title":"From the Archive: \"Reading the Room with Beethoven\"","date":"August 01, 2019","description":null,"public_tags":["TypeScript"]},"body":"\nTODO: Introduction\n\nFrom the [original submission](https://devpost.com/software/beethoven-t9ud86):\n\n> ## Inspiration\n> Spencer, one of our team members, has a parent who is hearing-impaired and another that provides live, remote, ver batem transcription services for deaf and hard-of-hearing undergraduate, Ph.D., and Executive MBA students across the nation. Dan and Joseph, two of our other members, work for Sorenson Communications making video calling and other applications for the deaf community. We noticed that, for deaf students who would prefer live captioning in their lectures, there is no one-stop solution for both audio streaming and live captioning. We created Beethoven, an application for quick and accurate transcription in the classroom, to solve this.\n> \n> ## Functionality\n> Put simply, Beethoven will pair two devices together - an audio input and a screen output - and stream captioning from one device to the other in real time. The student can put one device at the front of the class for the clearest possible audio, and then position another next to them to read live captioning of the lecture and dialog as if it was the Closed Captioning on a television show or movie. This can either be done through Google Cloud transcription services or a live CART interpreter, depending on the level of accuracy desired and resources available.\n> \n> ## Development Process\n> The application is split into three main parts - the audio input, the transcription service, and the screen output. First, the audio is captured by the audio input device. From there, it goes to either the Google Cloud voice transcriber, or a live CART interpreter. This happens through Google Cloud Services or peer-to-peer WebRTC, respectively. Once complete, the text will be sent to both our server's backend to be compiled into a transcript and send to the screen output device, placed in front of the student, where a stream of the captioning is displayed.\n> \n> ## What's next for Beethoven\n> Moving forward, we'd like to fully implement the CART reporting features of the application. The P2P handshaking feature has a lot of improvements that can be made. Once finished, we'd like to reach out to the deaf and CART communities to get their feedback on the product."},"previous":null,"next":{"fields":{"slug":"/supply-chain-analysis/"},"frontmatter":{"title":"From the Archive: \"What *are* Supply Chains, Anyway?\""}}},"pageContext":{"id":"380ff55b-2278-56c9-877f-27c69bf1481e","previousPostId":null,"nextPostId":"b4d4257d-43bc-5a81-b7f6-be6a0a675401","frontmatter":{"title":"From the Archive: \"Reading the Room with Beethoven\"","date":"2019-08-01","public_tags":["TypeScript"],"private_tags":["Presentation","Software","Hackathon","Google Cloud","Node","WebRTC","JavaScript","Deaf","Transcription","CART","Stenography","Plover"],"visible":false}}},"staticQueryHashes":["2841359383","2929475149","4004405182"],"slicesMap":{}}